{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945b8068-2358-4823-a595-ea2a75e4e543",
   "metadata": {},
   "source": [
    "# Machine learning for Chemistry - tutorial\n",
    "\n",
    "## Project example 1  - Recurrent Neural networks\n",
    "\n",
    "In this computer tutorial, we will build a recurrent neural network (RNN) to generate molecular structures. Several topics that we have learned throughout this course will come together in this assignment, including:\n",
    "* SMILES codes to represent molecules,\n",
    "* the RDKit library to draw molecular structures,\n",
    "* the cross-entropy, which we will use for the loss function,\n",
    "* PyTorch to design the neural network.\n",
    "\n",
    "The main new concepts that we will learn in this tutorial are: \n",
    "1. building, training, and applying a recurrent neural network, \n",
    "1. using a neural network for generative modeling.\n",
    "\n",
    "\n",
    "RNNs are powerful neural network architectures that can deal with sequential data and with time series. In contrast to a vanilla feed-forward neural network in which an input data point, $x_i$, transfers from the first hidden layer to the last hidden layer to result in an output $y_i$, in a recurrent neural network, an input $x_i$ is not only passed from the first hidden layer forward to the next layer, but also backward to work as input to the same hidden unit again. As a result, $x_i$ not only determines $y_i$, but also affects (together with $x_{i+1}$) the next output $y_{i+1}$, and also later output values. The network is said to have _memory_.\n",
    "\n",
    "A common application for RNNs is in natural language processing, text can be seen as a sequences of sentences, a sentence is a sequence of words, and a word is a sequences of letters. For a given language, there is a relation between the letters in a word, and between the words in a sentence. In other words, given the first three letters of a word, for example \"mac\", the probability for a possible fourth letters is not a flat distribution over the English alphabet.\n",
    "\n",
    "\n",
    "In this tutorial, we will use an RNN to learn the pattern in molecules and use that to construct new molecules. This is done using the SMILES representations of the molecules. A SMILES code is a sequence of letters and special tokens, which can be seen as words of a special language with a simple grammar.\n",
    "\n",
    "We train the network to predict the next character in SMILES strings based on the previous character and the current memory state. The loss function represents a penalty for the incorrect prediction for the next character. When the network is trained enough, we can use it to sample new compounds. For sampling, we just put a <BOS> (begin of sequence) symbol as the initial token, and the network generates the next token. At the next step, we use this token as input and repeat the process to generate the full sequence of tokens. \n",
    "\n",
    "Tasks to fulfill:\n",
    "* build a RNN with pytorch\n",
    "* learn how to save and load the state of a (partially) trained neural network\n",
    "* generate new SMILES codes\n",
    "* visualize the generated molecules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad90e00-c1d0-4d66-89b6-b9afc3518a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first we load some useful libraries\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import one_hot\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from rdkit import RDLogger\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "__special__ = {0: \"<PAD>\", 1: \"<BOS>\", 2: \"<EOS>\"}\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c84097-e483-4dc6-90e4-f1c852969616",
   "metadata": {},
   "source": [
    "The class “SmilesProvider” inherits the PyTorch DataLoader class. At initialization, it \"tokenizes\" all molecules from a dataset and builds a dictionary with indexes for each SMILES token. Here a token is the smallest unit  used to make Smiles \"words\" and is made from 1 or 2 characters, such as \"c\", \"C\", \"Si\", etc.\n",
    "\n",
    "In the following cell the “SmilesProvider” class is defined. We will just make of the class as is, e.g. to read the training set. \n",
    "\n",
    "$\\color{DarkBlue}{\\textbf{Task:}}$\n",
    "* try to understand which <code>file</code> this class reads. This file is provided in the Canvas Module. Download and add this file to the same folder as this jupyter file. Open the file in jupyter lab or in a text editor, to see what is in this file.\n",
    "* execute the cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657d72ab-bc55-4776-8b2d-132063c5f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesProvider(torch.utils.data.DataLoader):\n",
    "    def __init__(self, file, total=130):\n",
    "        self.total = total\n",
    "        self.smiles = open(file, 'r').read().split(\"\\n\")[:-1]\n",
    "        tokens = functools.reduce(lambda acc,s: acc.union(set(s)), self.smiles ,set())\n",
    "        self.vocsize = len(tokens) + len(__special__)\n",
    "        self.index2token = dict(enumerate(tokens,start=3))\n",
    "        self.index2token.update(__special__)\n",
    "        self.token2index = {v:k for k,v in self.index2token.items()}\n",
    "        self.ints = [torch.LongTensor([self.token2index[s] for s in line]) for line in tqdm(self.smiles,\"Preparing of a dataset\")]\n",
    "\n",
    "    def decode(self,indexes):\n",
    "        return \"\".join([self.index2token[index] for index in indexes if index not in __special__])\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        special_added = torch.cat((torch.LongTensor([self.token2index['<BOS>']])\n",
    "                                   ,self.ints[i],torch.LongTensor([self.token2index['<EOS>']]),\n",
    "                                   torch.LongTensor([self.token2index[\"<PAD>\"]]*(self.total-len(self.ints[i])-2))),dim=0)\n",
    "        return one_hot(special_added,self.vocsize).float(),special_added\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80202a66-1a42-415b-9b8b-9c49416cd438",
   "metadata": {},
   "source": [
    "Also the following class, we will just use and run as is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bac67a5-9fa1-4a58-b68b-09b4fbe44ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesLSTM(nn.Module):\n",
    "    def __init__(self, vocsize, device, max_len=130, hidden_size=256, num_layers=1, num_units=3):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.vocsize = vocsize\n",
    "        self.max_len = max_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_units = num_units\n",
    "        self.lstm_1 = nn.LSTM(vocsize, hidden_size, bidirectional=False, batch_first=True, num_layers=num_layers, dropout = 0.5)\n",
    "        self.lstm_2 = nn.LSTM(hidden_size, hidden_size, bidirectional=False, batch_first=True, num_layers=num_layers, dropout = 0.3)\n",
    "        self.linear = nn.Linear(hidden_size, vocsize)\n",
    "\n",
    "# This doesnt do anaything it just selctects the first caracter and uses that. Also not used?\n",
    "    def forward(self, x):\n",
    "        # if hc == None:\n",
    "        #     h_1 = torch.zeros((self.num_layers, x.size(0), self.hidden_size)).to(self.device)\n",
    "        #     c_1 = torch.zeros((self.num_layers, x.size(0), self.hidden_size)).to(self.device)\n",
    "        #     h_2 = torch.zeros((self.num_layers, x.size(0), self.hidden_size)).to(self.device)\n",
    "        #     c_2 = torch.zeros((self.num_layers, x.size(0), self.hidden_size)).to(self.device)\n",
    "        # else:\n",
    "        #     h_1, c_1, h_2, c_2 = hc\n",
    "\n",
    "        # accumulator = torch.zeros(x.size(0), self.max_len)\n",
    "        # for i in range(self.max_len):\n",
    "        x, _ = self.lstm_1(x)\n",
    "        x, _ = self.lstm_2(x)\n",
    "        x = self.linear(x)\n",
    "        # print(\"before\", x.size())\n",
    "        # x = F.softmax(x, dim=1)\n",
    "        # print('softmax', x.size())\n",
    "        # x = torch.multinomial(x,num_samples=1,replacement=True).squeeze(1)\n",
    "        # print(\"multinomial\", x.size())\n",
    "        # accumulator[:,i] = x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample(self,batch_size=128):\n",
    "        bos_token = [k for k,v in __special__.items() if v == \"<BOS>\"][0]\n",
    "        x = torch.LongTensor([bos_token]*batch_size)\n",
    "        h_1 = torch.zeros((self.num_layers, batch_size, self.hidden_size)).to(self.device)\n",
    "        c_1 = torch.zeros((self.num_layers, batch_size, self.hidden_size)).to(self.device)\n",
    "        h_2 = torch.zeros((self.num_layers, batch_size, self.hidden_size)).to(self.device)\n",
    "        c_2 = torch.zeros((self.num_layers, batch_size, self.hidden_size)).to(self.device)\n",
    "        accumulator = torch.zeros(batch_size, self.max_len)\n",
    "        for i in range(self.max_len):\n",
    "\n",
    "            x = one_hot(x, self.vocsize).float().unsqueeze(1).to(self.device)\n",
    "            x, (h_1, c_1) = self.lstm_1(x, (h_1, c_1))\n",
    "            x, (h_2, c_2) = self.lstm_2(x, (h_2, c_2))\n",
    "            x = self.linear(x).squeeze(1)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            x = torch.multinomial(x, num_samples=1,replacement=True).squeeze(1)\n",
    "            accumulator[:,i] = x\n",
    "        return accumulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fae8c-fe05-4061-af7d-4826418ad0d2",
   "metadata": {},
   "source": [
    "## Generating Smiles strings\n",
    "\n",
    "The <code>generate()</code> function generates Smiles strings.\n",
    "\n",
    "$\\color{DarkBlue}{\\textbf{Task:}}$\n",
    "* Use the RDKit function <code>Chem.MolFromSmiles()</code> to check if the produced Smiles is valid (it returns \"None\" on failure) and increase the variable <code>correct</code> otherwise.\n",
    "* Set the PyTorch model in the <code>eval</code> modus in the correct place. (Note that this can be anywhere in the cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "009be3a5-cd38-4299-98df-d4834e1b8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(file='genmodelLSTM.pt',batch_size=64):\n",
    "    \"\"\"\n",
    "    This is the entrypoint for the generator of SMILES\n",
    "    :param file: A file with pretrained model\n",
    "    :param batch_size: The number of compounds to generate\n",
    "    :return: None. It prints a list of generated compounds to stdout\n",
    "    \"\"\"\n",
    "    box= torch.load(file)\n",
    "    model,tokenizer = box['model'],box['tokenizer']\n",
    "    model.eval()\n",
    "    res = model.sample(batch_size)\n",
    "    correct = 0\n",
    "    list_smiles = []\n",
    "    for i in range(res.size(0)):\n",
    "        smiles = \"\".join([tokenizer[index] for index in res[i].tolist() if index not in __special__])\n",
    "        # print(smiles)\n",
    "# ======== start your code here =================================\n",
    "        if Chem.MolFromSmiles(smiles) != None:\n",
    "            correct += 1\n",
    "            list_smiles.append(smiles)\n",
    "# ======== end your code here ===================================\n",
    "    # print (\"% of correct molecules is {:4.2f}\".format(correct/float(batch_size)*100))\n",
    "    return list_smiles, correct/float(batch_size)*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c0973-f319-46f9-8041-3c562f36b56d",
   "metadata": {},
   "source": [
    "## Training the RNN\n",
    "\n",
    "The recurrent neural network learns valid Smiles strings from the training data set of Smiles examples. However, running the training takes a lot of time, een on a fast GPU. \n",
    "\n",
    "We will therefore not actually perform the training here. Instead, we will load a trained state of the model and use that to do predictions.\n",
    "\n",
    "Nevertheless, let's have a look at the training function <code>train</code> below, and fill in a number of blancs in the code.\n",
    "\n",
    "$\\color{DarkBlue}{\\textbf{Task 1:}}$\n",
    "In the relevant section below, add the following:\n",
    "* set the <code>optimizer</code> variable to the \"Adam\" optimizer of Pytorch\n",
    "* set the <code>loss_function</code> variable to a cross-entropy loss function from PyTorch nn module\n",
    "* switch the PyTorch optimizer in the \"train\" mode.\n",
    "\n",
    "$\\color{DarkBlue}{\\textbf{Task 2:}}$\n",
    "In the relevant sections below, add in the correct spots:\n",
    "* set the gradiants of the PyTorch optimizer to zero\n",
    "* compute the backprop gradients for the loss function\n",
    "* do a step with the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2838e671-e5c9-4c7b-87a8-53085b1c2fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing of a dataset: 100%|██████████| 249456/249456 [00:01<00:00, 127157.27it/s]\n",
      "Training: 100%|██████████| 975/975 [01:30<00:00, 10.73it/s]\n",
      "Training: 100%|██████████| 975/975 [01:32<00:00, 10.58it/s]\n",
      "Training: 100%|██████████| 975/975 [01:32<00:00, 10.50it/s]\n",
      "Training: 100%|██████████| 975/975 [01:33<00:00, 10.47it/s]\n",
      "Training: 100%|██████████| 975/975 [01:33<00:00, 10.48it/s]\n",
      "Training: 100%|██████████| 975/975 [01:33<00:00, 10.44it/s]\n",
      "Training: 100%|██████████| 975/975 [01:33<00:00, 10.39it/s]\n",
      "Training: 100%|██████████| 975/975 [01:34<00:00, 10.37it/s]\n",
      "Training: 100%|██████████| 975/975 [01:33<00:00, 10.40it/s]\n",
      "Training: 100%|██████████| 975/975 [01:33<00:00, 10.41it/s]\n"
     ]
    }
   ],
   "source": [
    "def train(file='250k.smi',batch_size=256,learning_rate=0.001,n_epochs=10,device='cuda'):\n",
    "    \"\"\"\n",
    "    This is the entrypoint for training of the RNN\n",
    "    :param file: A file with molecules in SMILES notation\n",
    "    :param batch_size: A batch size for training\n",
    "    :param learning_rate: A learning rate of the optimizer\n",
    "    :param n_epochs: A number of epochs\n",
    "    :param device: \"cuda\" for GPU training, \"cpu\" for training on CPU, if there are no CUDA on a computer it uses CPU only\n",
    "    :return: None. It saves the model to \"genmodel.pt\" file\n",
    "    \"\"\"\n",
    "    device = device if torch.cuda.is_available() else 'cpu'\n",
    "    dataset = SmilesProvider(file)\n",
    "    model = SmilesLSTM(dataset.vocsize, device=device).to(device)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# ======== TASK 1 start your code here =================================\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "# ======== TASK 1 end your code here ===================================\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for iteration,(batch,target) in enumerate(tqdm(dataloader,'Training')):\n",
    "            batch, target = batch.to(device), target.to(device)\n",
    "            out = model(batch)\n",
    "            out = out.transpose(2,1)\n",
    "            # what does the slcing do it removes the last from prediction and the first carater from the target\n",
    "            loss = loss_function(out[:,:,:-1],target[:,1:])\n",
    "            # print('loss', loss)\n",
    "            # print('batch', batch)\n",
    "            # print(\"prediction\", out.size())\n",
    "            # print(\"prediction\", out[0,:,:])\n",
    "\n",
    "            # print(\"target\", target.size())\n",
    "            # print(\"target\", target[0,:])\n",
    "\n",
    "# ======== TASK 2 start your code here =================================\n",
    "            optimizer.zero_grad()\n",
    "# ======== TASK 2 end your code here ===================================\n",
    "            loss.backward()\n",
    "# ======== TASK 2 start your code here =================================\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "# ======== TASK 2 end your code here ===================================\n",
    "    model.device = 'cpu'\n",
    "    torch.save({'tokenizer':dataset.index2token,'model':model.cpu()},\"genmodelLSTM.pt\")\n",
    "train() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e35118-47f5-4d24-ba84-be9f084e2553",
   "metadata": {},
   "source": [
    "## Generating molecules\n",
    "\n",
    "$\\color{DarkBlue}{\\textbf{Task:}}$\n",
    "* call the <code>generate()</code> function to produce a list of molecules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bdd2c8f-82b7-46bc-a168-a46b1f18df0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.625 %\n"
     ]
    }
   ],
   "source": [
    "# ======== start your code here =================================\n",
    "s, c = generate()\n",
    "print(c, \"%\")\n",
    "# ======== end your code here ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761eff9-cc23-4aee-af34-2f80146e1ab3",
   "metadata": {},
   "source": [
    "## Visualizing molecules\n",
    "\n",
    "$\\color{DarkBlue}{\\textbf{Task:}}$\n",
    "* Use RDKit to draw some of the produced chemical structures to assess how realistic they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3078be70-e865-4162-98e3-4a8fb3dc3bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O=c1[nH]c(-c2ccc(Cl)cc2)nc2sc(N3CCCC3)nc12', 'C[C@@H](NC(=O)N[C@@H]1CCCCC[C@@H]1O)c1ccc(F)c(Cl)c1', 'O=C(Cn1nnc2ccccc2c1=O)N1CCC(c2cccc(C(F)(F)F)c2)CC1', 'Cc1cc(C(=O)NN(C2CCCCC2)[C@@H]2CCS(=O)(=O)C2)ccn1', 'N#C[C@]1(N)CCC[C@@H](Sc2ccc(F)cc2)C1', 'C[C@]1(NC(=O)c2cc3c(s2)CCC3)CCSCC[C@@H]1C(C)(C)C', 'CCc1nc(-c2ccncc2)nc(N2CCCCC2)n1', 'Cc1cccc(CN(C2CC2)[C@@H](CS(C)(=O)=O)c2ccc(F)cc2)c1', 'CCCCOc1ccc(Br)cc1C[NH2+]C(C)C', 'COc1ccc(OC)c(NS(=O)(=O)c2ccc(C(=O)NCc3nccc(-c4ccccc4)c3)s2)c1', 'CC[C@H](Cn1cc[nH+]c1)NC(=O)c1ccoc1C', 'CC[C@@H](Oc1ccc(OC)cc1)C(=O)Nc1cccc([N+](=O)[O-])c1', 'COc1cccc(CNC(=O)C[NH+](C)[C@H]2CCSc3ccccc32)c1', 'Cc1csc([C@@H](C#N)c2ccc([N+](=O)[O-])cc2Cl)n1', 'CC(C)n1nccc1N[C@@H](C)c1nc(Cc2cccs2)no1', 'CCN(CC)C(=O)c1cc(C(=O)N2CCc3c(O)cccc32)sc1C', 'O=c1[nH]ncn1CCN1CCO[C@@H](c2ccc3ccccc3c2)C1', 'CN(C)C(=O)Cc1cnc(NC[C@@H]2CCCO2)s1', 'COc1ccc(NC(=O)Nc2cc(C)ccc2C)c(C)c1', 'COc1ccccc1NC(=O)N1CCN(C[C@@H](C)O)CC1', 'Cn1ncnc1C(=O)[C@H](C#N)c1ccccc1', 'CC[C@@H](C)[NH2+]Cc1cc(F)c(S(N)(=O)=O)cc1', 'Cc1ccc([C@H](CNC(=O)N2CC[C@@H](CC3CC3)C2)[NH+](C)C)o1', 'CCNC(=O)c1ccc(NC(=O)[C@H](C)c2nc(C)nc(N)c2Cl)cc1', 'CC(C)[C@@H]([NH3+])c1nc(C2CC2)ns1', 'C[NH+]1CCN(C(=O)C(=O)N2CCC(OC3CCCC3)CC2)CC1', 'CC[C@@H]1C[NH+]2CCC[C@@H]2CN1Cc1cc(=O)c(O)c(C)s1', 'CCC[NH+]1CCC[C@@H]1[C@@H]1COCCS1', 'C[S@@](=O)c1ccc(C(=O)NCc2ccccc2Cl)cn1', 'C[C@@H](N[C@@H](C)C(=O)N[C@@H]1CN(Cc2cccc(F)c2)C1=O)c1cnn(C)c1', 'O=C(C1CCC(Oc2ccccc2)CC1)Nc1ccncc1', 'CSc1ccc([C@H](O)[C@H](C)NC(=O)c2ccc(C#N)nc2)cc1', 'COC(=O)[C@H](C)[C@H]([NH3+])c1ccc(C)cc1', 'CCN(CC)C(=O)[C@H](C)NS(=O)(=O)c1ccc(Cl)cc1', 'COc1ccc(OC)c(N[C@H](C(N)=O)C(C)(C)C)c1', 'C[C@H](COC1CCOCC1)n1nnc(-c2ccc(F)cc2)n1', 'O=C(NCCN1CCN(C(=O)Cc2ccc(F)cc2)CC1)c1ccccc1', 'Cc1cc(C)n(Cc2ccc(NCCc3ccccc3)cc2)n1', 'Cc1cccc(Cl)c1NC(=O)c1[nH]nc(C2CC2)c1[N+](=O)[O-]', 'C[C@]1(c2ccccc2Cl)NC(=O)N(CC(=O)NCc2ccccc2)c1=S', 'CC[NH+](CC(=O)NC(C)C)c1nc2c(s1)CN(C(=O)CNc1cccc([N+](=O)[O-])c1C)CC2', 'Cc1ccnc([C@@H](NC(=O)c2ccc(C(=O)OC)cc2)C2CC2)c1', 'COc1ccc(Br)cc1C(=O)N1C[C@H](C)O[C@H](C)C1', 'CC(C)C(=O)Nc1ccc(Cl)c(C(=O)[O-])c1', 'COCCCNC(=O)[C@H]1CS(=O)(=O)C[C@@H]1N1C[C@H](C)OCC1=O', 'Cc1ccc(NC(=O)c2cc([N+](=O)[O-])ncn2)s1', 'Cc1cscc1C(=O)N1CCC[C@H]1c1nc2ccccc2s1', 'COc1ccc(NC(=O)NCCN2CCO[C@H](c3ccc(F)cc3)C2)cc1', 'CS(=O)(=O)c1ccc(C(=O)Nc2ccccc2F)cn1', 'C[NH+]1CCCC[C@@H]1C(=O)CCCCCO', 'Cc1cc(NC(=O)COc2ncnc3onc(C)c23)o[nH+]1', 'C[C@@H](CC(=O)[O-])CC(=O)N[C@@H]1CCSc2ccccc21', 'CC[C@H](C)NC(=O)c1cc(C)nc2c1c(C)nn2-c1ccccn1', 'Cc1ccc(C(=O)NC2CCN(C(=O)c3ccccc3F)CC2)cc1', 'C[CH]1C[C@@H]1C(=O)N1CC[C@@H](C(=O)N(C)CC(=O)Nc2ccccc2Cl)C1', 'Clc1ccnc(C[C@@H]2C[C@H]2c2ccccc2)[n+]1[C@H]1CCOC1']\n"
     ]
    }
   ],
   "source": [
    "# ======== start your code here =================================\n",
    "batch_size = 64\n",
    "smiles, pc = generate(batch_size=batch_size)\n",
    "n_print = 9\n",
    "len_smiles = len(smiles)\n",
    "indicies = []\n",
    "mols = []\n",
    "n = 0\n",
    "if len_smiles >= n_print:\n",
    "\n",
    "    while n < len_smiles:\n",
    "        index = n-1\n",
    "        if index not in indicies:\n",
    "            indicies.append(index)\n",
    "            mols.append(Chem.MolFromSmiles(smiles[index]))\n",
    "            n += 1\n",
    "\n",
    "    Draw.MolsToGridImage(mols)\n",
    "    print(smiles)\n",
    "\n",
    "# ======== end your code here ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb696a-0616-4ef0-aeea-d745a0a1cb36",
   "metadata": {},
   "source": [
    "$\\color{DarkBlue}{\\textbf{Question 1:}}$ How can the RNN model be further improved to generate useful molecules with certain desired properties?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd066b-b11c-4c87-b1af-0e730fdec893",
   "metadata": {},
   "source": [
    "$\\color{Grey}{\\textsf{<double-click and type your answer here>}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74dda8-d259-461d-9f9b-518991bd879e",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This ends this part of the final ML4Chem Tutorial on Advanced Neural Networks.\n",
    "\n",
    "In this tutorial, we have used a RNN to generate new molecules from Smiles strings. Apart from obtaining a deeper understanding of the possibilites of neural networks, the PyTorch library, saving and loading trainined networks, we hope that this tutorial has also inspired you to think of other possibilities and applications of machine learning for chemistry! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ec7ea-88f7-4ab9-b887-fab2e115bff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4770c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e80f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
