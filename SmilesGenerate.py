"""

This module conatians the main output routines from the model. This includes the base geenration of SMILES and generating SMILES after RL training. All routines can be used as final functions but are also called by other modules.

"""

import torch
import torch.nn.functional as F
import torch.nn as nn
from rdkit import Chem
import numpy as np
from tqdm import tqdm
from rdkit.Chem import Descriptors
from SmilesData import __special__, logger
from Smilesfeatures import MACCS_Tanimoto


def generate(file='SmilesLSTM_CHEMBL_22_50_epoch.pt', batch_size=64, temp=1., h=None, c=None):
    """
    The main generation function. This function reads in a model from file and calls the sample function (model.sample), it passes the batch size and the temp (could also pass c and h but not currently used). This function first calls the sample function of the model and then checks if the generated SMILES are valid SMILES using RDKit. The percentage of valid SMILES is also calculated.

    Args:
        file (str, optional): _description_. Defaults to 'SmilesLSTM_CHEMBL_22_50_epoch.pt'.
        batch_size (int, optional): The number of SMILES that should be sampled. This is not the number of valid SMILES that will be generated, wich is varible and is depened on the model. Defaults to 64.
        temp (_type_, optional): The temprature of the model. This influecnes the creativity of the model. Defaults to 1..
        h (None or torch.tensor(number of layers, batch size, hidden size), optional): The initial hidden state. Not used in this implementation. None should be used so that the model.sample function int this tensor. Defaults to None.
        c (None or torch.tensor(number of layers, batch size, hidden size), optional): The cell hidden state. Not used in this implementation. None should be used so that the model.sample function int this tensor. Defaults to None.

    Returns:
        list_smiles (python list): A list containing the valid SMILES generated by the model.
        p_valid_smiles (float): A % of the SMILES that were valid SMILES.
        h: Same as input but not currently used.
        c: S`ame as input but not currently used.
    """
    file = f"models/{file}"
    box= torch.load(file)
    model,tokenizer = box['model'],box['tokenizer']
    model.eval()
    res, h, c = model.sample(batch_size, temp=temp, h=h, c=c)


    correct = 0
    list_smiles = []
    #  Converting the indexed torch.tensors into str using the index2token(tokenizer).
    for i in range(res.size(0)):
        smiles = "".join([tokenizer[index] for index in res[i].tolist() if index not in __special__])
        
    # Checking if the smapled SMILES are valid SMILES.
        if Chem.MolFromSmiles(smiles) != None: # type: ignore
            correct += 1
            list_smiles.append(smiles)

    # calc the % valid SMILES.
    p_valid_smiles = correct/float(batch_size)*100

    return list_smiles, p_valid_smiles, h, c

def generate_current_model(model, tokenizer, batch_size=100, temp=1., device="cuda"):
    """
    Very similair to the generate function above but now the model and tokinizer are passed instead of read in. This alows us to generate SMILES during training. This function is mainly used to check the % valid SMILES that are generated each epoch.

    Args:
        model (torch model): The model
        tokenizer (SmilesProvider index2token): The tokinizer (index2token)
        batch_size (int, optional): The number of SMILES that should be sampled. This is not the number of valid SMILES that will be generated, wich is varible and is depened on the model. Defaults to 100.
        temp (float, optional): The temprature of the model. This influecnes the creativity of the model. Defaults to 1..
        device (str, optional): The device that should be used can be CPU or CUDA (GPU). Defaults to "cuda".

    Returns:
        list_smiles (python list): A list containing the valid SMILES generated by the model.
        p_valid_smiles (float): A % of the SMILES that were valid SMILES.
        indexed_smiles (torch.tensor) same as list_smiles but then in inex from. this is passed as this can be direcly used during the RL training instead of converting to and from str.
    """
    model.eval()
    res, _, _ = model.sample(batch_size, temp=temp)
    correct = 0
    list_smiles = []
    indecies = []
    #  Converting the indexed torch.tensors into str using the index2token(tokenizer).
    for i in range(res.size(0)):
        smiles = "".join([tokenizer[index] for index in res[i].tolist() if index not in __special__])

        # Checking if the smapled SMILES are valid SMILES.
        if Chem.MolFromSmiles(smiles) != None: # type: ignore
            correct += 1
            list_smiles.append(smiles)
            indecies.append(i)

    # Getting the indexed smiles from the indicies
    indexed_smiles = res[indecies]
    
    # calc the % valid SMILES.
    p_valid_smiles = correct/float(batch_size)*100
    
    return list_smiles, p_valid_smiles, indexed_smiles

def RL_training_generation(file='SmilesLSTM_CHEMBL_22_50_epoch.pt', propertie="logp", target_logp= 6., batch_size=1000, num_loop=5, temp=1., device="cuda", target_mol="", lr=0.001):
    """
    The RL training and generation of SMILES. First the base model is read in. then in a loop SMILES are generated , scored and both are passed to the RL_train function (where the training including rewards are used). It returns SMILES wich are generated after the RL train loop.

    Args:
        file (str, optional): The path to the model file. this model should be pre trained. Defaults to 'SmilesLSTM_CHEMBL_22_50_epoch.pt'.
        propertie (str, optional): Choice wich property is used for the RL training. for now can choice between logp (partition coeficient) and MACCS (similarity to target molecuel). Defaults to "logp".
        target_logp (float, optional): The target value of the logp RL traning. This vlaue is used to calcualte the reward. this vlaue is only used when propertie is set to logp. Defaults to 6..
        batch_size (int, optional): The number of SMILES generated each loop. This number is passed to the model.sample so this doesnt reflect the actuyal number of SMILES that is sued during the RL training. Defaults to 1000.
        num_loop (int, optional): The number of times new SMILES should be generated and learned (wich rewards). Defaults to 5.
        temp (float, optional): The temparture wich controls the creativity of the model. Defaults to 1..
        device (str, optional): The device to be used can be CPU or CUDA (GPU). Defaults to "cuda".
        target_mol (str, optional): The target moelcuel used during the MACCS RL training. This vlaue is only used when propertie is set to MACCS. Defaults to "".
        lr (float, optional): The elarning rate. this is direclty passed tot the RL_train function. This should be smaller than waht was used during training Defaults to 0.001.

    Returns:
        smiles (python list): THe SMILES generated after the RL training.
        pc (float): % valid smiles after the RL training.
        
        {file}_pc_data (save file txt): table containing the % valid SMILES throughout the RL training.
    """
    device = device if torch.cuda.is_available() else 'cpu'
    file = f"models/{file}"
    box = torch.load(file)
    model, tokenizer = box['model'], box['tokenizer']
    # making sure the model is send to the device
    model.device = device
    model.to(device)

    # generating SMILES with the current model
    smiles, cp, indexed_smiles = generate_current_model(model, tokenizer, batch_size=batch_size, temp=1., device=device)
    indexed_smiles.to(device)
    
    # Array containing the % valid SMILES
    pc_array = np.zeros(num_loop)
    # The actual traning loop
    for i in tqdm(range(1, num_loop+1), "Bias training"):

        # calculating the reard
        property_score = np.zeros(len(smiles))
        if propertie == "logp":
            for j, smile in enumerate(smiles):
                # Calculate the logp
                mol = Chem.MolFromSmiles(smile) # type: ignore
                property_score[j] = Descriptors.MolLogP(mol) # type: ignore
            # Calc the loss with target value
            rewards = np.abs(target_logp - property_score)
            # Scale the rewards {0.5 ; 1.5}
            rewards = (rewards - rewards.min()) / (rewards.max() - rewards.min()) + 0.5
            rewards = torch.from_numpy(rewards)
        elif propertie == "MACCS":
            # calculate the MACCS tanimoto simulairity 
            for j, smile in enumerate(smiles):
                property_score[j] = MACCS_Tanimoto(smile, target_mol)
            # Scaling the rewards {0.5 ; 1.5}
            rewards = torch.from_numpy(1 - property_score + 0.5)
            
        # Calling the RL train routine to train the model with the generated SMIELS and corespoding rewards
        total_loss = RL_train(model, indexed_smiles, rewards, lr=lr, device=device)
        
        # Checking the model after the RL training. Cheock for % valid SMILES 
        smiles, pc, indexed_smiles = generate_current_model(model, tokenizer, batch_size=batch_size, temp=1.)
        # Saving the % valid SMILES
        pc_array[i-1] = pc
        # Printing and logging the message cotaining the % vlaid SMILES and the loss.
        message = f"Bias traning it {i} of {num_loop} done of the bias training epoch loss: {total_loss}, reward mean {rewards.mean()}, valid smiles {pc}%"
        print(message)
        logger(message, "models/bias_training_logs") 
        
    # Generate the SMILES after the RL training 
    smiles, pc, indexed_smiles = generate_current_model(model, tokenizer, batch_size=batch_size, temp=1.)
    # Save the % valid smiles array for easy plotting
    np.savetxt(f"{file}_pc_data", pc_array)
    return smiles, pc
 
        
def RL_train(model, index_smiles, rewards, lr=0.001, n_epochs=10, device="cuda"):
    """
    The trainings loop used during RL training. This is a copy of the tranings function from Smilestrain with a reward added to the loss.

    Args:
        model (torch model): The model on wich the RL traning takes place
        index_smiles (torch.tensor): The generated SMILES on wich the model should be trained. The SMILES are index encoded as this is what the model excepts.
        rewards (torch.tnesor)): The tnesor containing the the reward (score). the indecies coresponde to the indicies of the indexed_smiles tensor. this realtion must be kept during training.
        lr (float, optional): Leanring rate. This should be lower than was used durning the base training. Defaults to 0.001.
        n_epochs (int, optional): Number of epoch (times the model is trained on the same data + rewards). Defaults to 10.
        device (str, optional): The device that should be used can be CPU or CUDA (GPU). Defaults to "cuda".

    Returns:
        Total loss (float): The total loss of all epochs.
        
        The model is trained and is alos implicictly returned.
    """
    device = device if torch.cuda.is_available() else 'cpu'
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_function = nn.CrossEntropyLoss(reduction="none")
    model.train()
    # Making sure the tensor is going to the device
    index_smiles = index_smiles.to(torch.int64).to(device)

    # Creates a one hot encoding of the indexed SMILES
    batch = F.one_hot(index_smiles, model.vocsize).float().to(device)
    total_loss = 0.
    
    #  The traning loop
    for epoch in range(1, n_epochs + 1):
        random_index = torch.randperm(index_smiles.size(0))
        index_smiles = index_smiles[random_index]
        batch = batch[random_index]
        rewards = rewards[random_index]
        
        out = model(batch)
        # The out tensor is (batch size, sequence length, unqiue tokens size) but the loss function expceptcts (batch size, unqiue tokens size, sequence length)
        out = out.transpose(2,1)
        #  the loss is calcualted from the 2 caracter of the target and until the one before last caracter of the prediction (out). This ensures that the loss is calcualted for the actual predicted caracter.
        loss = loss_function(out[:,:,:-1], index_smiles[:,1:])
        # Sum over the caracters of the SMILES string
        loss = loss.sum(dim=1)
        # Award reward by mul the loss (cost) function
        loss = loss * rewards.to(device)
        # sum the loss wich now include the reward
        loss = loss.sum()
        total_loss += loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    return total_loss


# Doesnt work gives empty strings. posible that because normaly the string should be filled with filler tokens to 130 that it just continius when h and c are used

# def generate_properties(file='SmilesLSTM30ep.pt',batch_size=10, num_loop=5, temp=1.):
#     h = None
#     c = None
#     target = 10.
#     for i in range(num_loop):
#         smiles, pc, h, c = generate(file=file, batch_size=batch_size, temp=temp, h=h, c=c)
#         property_score = np.zeros(len(smiles))
        
#         for j, smile in enumerate(smiles):
#             mol = Chem.MolFromSmiles(smile) # type: ignore
#             property_score[j] = Descriptors.MolLogP(mol) # type: ignore
#             best_index = np.absolute(property_score - target).argmin()

#         h = h[:, best_index, :].unsqueeze(dim=1).repeat(1, batch_size, 1)
#         c = h[:, best_index, :].unsqueeze(dim=1).repeat(1, batch_size, 1)

        
#     return smiles

   
            
        